{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Radiantloom Email Assist 7B"
      ],
      "metadata": {
        "id": "zwKAnL4p3yu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "!pip install -q transformers peft accelerate bitsandbytes safetensors sentencepiece langchain gradio sentence-transformers"
      ],
      "metadata": {
        "id": "IMZxKUwtvzQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline"
      ],
      "metadata": {
        "id": "T8Lh6avy-tMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model with 4bit quantization\n",
        "model_name = \"aigeek0x0/radiantloom-email-assist-7b\"\n",
        "\n",
        "def load_quantized_model(model_name: str):\n",
        "    \"\"\"\n",
        "    :param model_name: Name or path of the model to be loaded.\n",
        "    :return: Loaded quantized model.\n",
        "    \"\"\"\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        load_in_4bit=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def initialize_tokenizer(model_name: str):\n",
        "    \"\"\"\n",
        "    Initialize the tokenizer with the specified model_name.\n",
        "\n",
        "    :param model_name: Name or path of the model for tokenizer initialization.\n",
        "    :return: Initialized tokenizer.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.bos_token_id = 1  # Set beginning of sentence token id\n",
        "    return tokenizer\n",
        "\n",
        "# load model\n",
        "model = load_quantized_model(model_name)\n",
        "\n",
        "tokenizer = initialize_tokenizer(model_name)\n",
        "\n",
        "# define stop token ids\n",
        "stop_token_ids = [0]"
      ],
      "metadata": {
        "id": "fEg-xPX1-u0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# specifiy the prompt\n",
        "prompt = \"\"\"\n",
        "\n",
        "<s>[INST]\n",
        "<<SYS>>Given an input, provide a clear and concise summary and action items in less than 50 words.<</SYS>>\n",
        "\n",
        "Answer the question below from context below :\n",
        "\n",
        "Convert the summary into a voice memo, keeping it under one minute:\n",
        "\n",
        "From: deb@example.com\n",
        "To: tan@xyz.com\n",
        "Subject: Review and Feedback on Radiantloom Models for Business Integration\n",
        "\n",
        "Hi Tan,\n",
        "\n",
        "I hope this email finds you well. We are currently in the process of exploring Radiantloom models for potential integration into our business operations. Specifically, we are interested in your insights on the \"radiantloom-email-assist-7b\" model.\n",
        "\n",
        "To ensure that we make well-informed decisions, we kindly request that you take some time to thoroughly review the documentation provided for this particular model. Your expertise and perspective are valuable to us, and we would appreciate any insights or observations you can share regarding the features, capabilities, and potential benefits of the \"radiantloom-email-assist-7b\" model.\n",
        "\n",
        "If you come across any noteworthy points, concerns, or suggestions during your review, please don't hesitate to bring them to our attention. Your feedback will play a crucial role in shaping our decision-making process as we assess the suitability of Radiantloom models for our business needs.\n",
        "\n",
        "Thank you in advance for your time and effort in this matter. We look forward to hearing your thoughts and recommendations.\n",
        "\n",
        "Best regards,\n",
        "\n",
        "Deb\n",
        "\n",
        "\n",
        "[/INST]</s>\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zeKofdLa_JJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run inference\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "output = model.generate(input_ids, max_length=512,\n",
        "    temperature=0.1, repetition_penalty=1.1, top_p=0.7, top_k=50)\n",
        "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "tv8_OPiY-7W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain Implementation"
      ],
      "metadata": {
        "id": "9V6l5TH2RVgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing unicode error in google colab\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "s4tNGrpgRXKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure huggingface pipiline\n",
        "pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=1024,                        # Increase this upto 4096 with a more powerful GPUs.\n",
        "        temperature=0.1,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "cWvvLIIMXoRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain"
      ],
      "metadata": {
        "id": "5-qAPMZJRbuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for running the chain\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "\n",
        "def email_assist(prompt):\n",
        "\n",
        "  template = \"\"\"<s>[INST]\n",
        "  <<SYS>>\n",
        "  Given an input, provide a clear and concise summary and action items in less than 50 words.\n",
        "  <</SYS>>\n",
        "\n",
        "  Answer the question below from context below:\n",
        "  {context}\n",
        "  {question} [/INST] </s>\n",
        "  \"\"\"\n",
        "\n",
        "  question = \"\"\"Convert the summary into a voice memo, keeping it under one minute:\"\"\"\n",
        "\n",
        "\n",
        "  context = prompt\n",
        "\n",
        "  prompt = PromptTemplate(template=template, input_variables=[\"question\",\"context\"])\n",
        "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "  response = llm_chain.run({\"question\":question,\"context\":context})\n",
        "  return response"
      ],
      "metadata": {
        "id": "NYjgFv-AReYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Application"
      ],
      "metadata": {
        "id": "U-sjiFnNRmTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# launch gradio UI for inference\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def summarizer(text):\n",
        "    prompt = text\n",
        "    response = email_assist(prompt)\n",
        "    return response\n",
        "\n",
        "pred = gr.Interface(\n",
        "    summarizer,\n",
        "    title = \"Radiantloom Email Assist - Turn Emails to Voice Memos & Chat Messages\",\n",
        "    inputs=[\n",
        "        gr.Textbox(\n",
        "            label=\"Email Text\",\n",
        "            lines=1,\n",
        "            placeholder=\"\"\"\n",
        "\n",
        "From: deb@example.com\n",
        "To: tan@xyz.com\n",
        "Subject: Using Email Assistant Model\n",
        "\n",
        "Hi Tan,\n",
        "\n",
        "We are exploring Radiantloom models for our business.\n",
        "Please review their documentation for the \"radiantloom-email-assist-7b model and let me know what you think.\n",
        "\n",
        "Thanks.\n",
        "\n",
        "Best,\n",
        "Deb\n",
        "            \"\"\",\n",
        "        ),\n",
        "    ],\n",
        "    outputs='text',\n",
        ")\n",
        "\n",
        "pred.launch(share=True)"
      ],
      "metadata": {
        "id": "ifAWA86NRncF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}